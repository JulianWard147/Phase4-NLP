{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment\n",
    "\n",
    "This notebook was created as the final product of a project for Phase 4 of the Flatiron School Data Science course. Its authors are Julian Ward and Vu Brown.\n",
    "\n",
    "The goal of this project was to create a model to assess the emotional sentiment of a tweet, on behalf of a hypothetical tech compnay that wished to see how the public was reacting to their products. This task was undertaken to practice and display Natural Language Processing skills on a realistic task. \n",
    "\n",
    "## Bottom Line Up Front\n",
    "\n",
    "The final model created is moderately accurate in the aggregate, but is somewhat limited when focused on individual tweets. We believe this model would be able to assist a human PR team in catching overall trends in sentiment. Because we envision semi-regularly running the model on tweets relating to our client's product, we opted to use the computationally quick Multinomial Bayes Model. \n",
    "\n",
    "An additional use for this model would be to try to find negative tweets, to allow a PR team to quickly respond to potencial problem spots. While this model could provide a narrowed range of possible negative tweets to a PR team, human intervention would be required to sort out false negatives. The threshold for selecting negative tweets could be tweaked to fit with the capacity of our client to provide such human oversight. An alternate notebook exists which contains the skeleton of a model focused on just finding negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The dataset used in this project came from [Data World](https://data.world/crowdflower/brands-and-product-emotions). The dataset contains 9093 tweets that were sorted by human raters into a number of emotional sentiments. For the purposes of this project we sorted \"No emotion toward brand or product\" and \"I can't tell\" into a single category of 'neutral.'\n",
    "\n",
    "Although including neutral results in our data created a lot of noise, we opted to include that data because our client would have no way to scrub neutral sentiment out of their real world data. \n",
    "\n",
    "The dataset was helpfully well curated, with only one row throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  target  \n",
       "0                                      Negative emotion       2  \n",
       "1                                      Positive emotion       1  \n",
       "2                                      Positive emotion       1  \n",
       "3                                      Negative emotion       2  \n",
       "4                                      Positive emotion       1  \n",
       "...                                                 ...     ...  \n",
       "9088                                   Positive emotion       1  \n",
       "9089                 No emotion toward brand or product       0  \n",
       "9090                 No emotion toward brand or product       0  \n",
       "9091                 No emotion toward brand or product       0  \n",
       "9092                 No emotion toward brand or product       0  \n",
       "\n",
       "[9093 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding='unicode_escape')\n",
    "df['target'] = df.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "\n",
    "unique_emotions = df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts().index\n",
    "for emotion in unique_emotions:\n",
    "    if emotion == \"Positive emotion\":\n",
    "        df.target.replace(to_replace=emotion, value=1, inplace=True)\n",
    "    elif emotion == \"Negative emotion\":\n",
    "        df.target.replace(to_replace=emotion, value=2, inplace=True)\n",
    "    else:\n",
    "        df.target.replace(to_replace=emotion, value=0, inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      " 3   target                                              9093 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 284.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tweet_text                                                                           NaN\n",
       "emotion_in_tweet_is_directed_at                                                      NaN\n",
       "is_there_an_emotion_directed_at_a_brand_or_product    No emotion toward brand or product\n",
       "target                                                                                 0\n",
       "Name: 6, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info())\n",
    "display(df.iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9092 non-null   object\n",
      " 3   target                                              9092 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 355.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[df.tweet_text.notnull()]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5544\n",
       "1    2978\n",
       "2     570\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find very common words that begin with @ or #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text                                            @teachntech00 New iPad Apps For #SpeechTherapy...\n",
      "emotion_in_tweet_is_directed_at                                                                     NaN\n",
      "is_there_an_emotion_directed_at_a_brand_or_product                   No emotion toward brand or product\n",
      "target                                                                                                0\n",
      "Name: 5, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@teachntech00']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell can be used to pull @~~~ strings out of particular tweets. As an example, here we're looking at tweet 5.\n",
    "tweet_num = 5\n",
    "print(df.iloc[tweet_num])\n",
    "re.findall('@[a-zA-Z0-9]+', df.tweet_text.iloc[tweet_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell generates a list of hashtags that appear in the dataset. We've commented out the final lines because the list is quite long.\n",
    "# Uncommment that line if you'd like to see the resulting list or set. \n",
    "\n",
    "list_hashtags = []\n",
    "for num in range(0, df.shape[0]):\n",
    "    temp = []\n",
    "    temp = re.findall('#[a-zA-Z0-9]+', df.tweet_text.iloc[num])\n",
    "    temp = list(set(temp))\n",
    "    if len(temp) > 0:\n",
    "        list_hashtags.extend(temp)\n",
    "\n",
    "#display(list_hashtags)\n",
    "#display(set(list_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ats = []\n",
    "for num in range(0, df.shape[0]):\n",
    "    temp = []\n",
    "    temp = re.findall('@[a-zA-Z0-9]+', df.tweet_text.iloc[num])\n",
    "    temp = list(set(temp))\n",
    "    if len(temp) > 0:\n",
    "        list_ats.extend(temp)\n",
    "\n",
    "#display(list_ats)\n",
    "#display(set(list_ats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this dataset, we opted to add 'sxsw' and 'mention' to our stopwords list, as they appear so often that it is likely that they contain little informational value. Further time might allow a more systematic approach to culling overused hashtags and @'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Model Building\n",
    "\n",
    "This section of the notebook displays a slightly condensed sample of the process used to arrive at our final model. Scroll down to the 'Final Model Analysis' to get directly to our results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating this stopwords list to use in later models.\n",
    "\n",
    "nltk_sw = stopwords.words('english')\n",
    "nltk_sw.extend(['sxsw', 'mention'])\n",
    "\n",
    "#Uncomment this to display the full list of stopwords.\n",
    "#nltk_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a train test split from our data.\n",
    "\n",
    "X = df.tweet_text\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a dictionary and a function to add the cross validation scores of our models \n",
    "# into that dictionary.\n",
    "\n",
    "cvdict = {}\n",
    "def reportcrossval(model, modelname, xtrain, ytrain):\n",
    "    cvmean = cross_val_score(model, xtrain, ytrain).mean()\n",
    "    cvdict[modelname] = cvmean \n",
    "    print(f\"The cv mean of {modelname} is {cvmean}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dummy Model\n",
    "\n",
    "This model was created to generate a baseline to compare our actual models to. We opted for the 'most frequent' strategy for simplicity. As you can see, the baseline accuracy we should expect if we simply assigned each tweet to the 'neutral' sentiment is around 61%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dummy = Pipeline([('dummy', DummyClassifier(strategy = 'most_frequent'))] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing dummy, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing dummy, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing dummy, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing dummy, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing dummy, total=   0.0s\n",
      "The cv mean of Dummy is 0.6137264056559728\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(pipeline_dummy, 'Dummy', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer and Multinomial Naive Bayes\n",
    "\n",
    "Here we experimented with the Count Vectorizer and the Multinomial Naive Bayes model.\n",
    "\n",
    "A Count Vectorizer takes in the text we provides it, does some preprocessing, applies a basic tokenizer, and finally 'counts' the appearances of words or n-grams in a bag of words, then turns that count in to a vector that we can put into our model.   \n",
    "\n",
    "A Multinomial Naive Bayes takes that vectorized data and applies Bayes' theorum to generate predictions. It is called 'naive' because it assumes that the n-grams are appearing independently of each other, which is not actually true, but can provide a computationally quick and reasonably accurate estimate nonetheless. Using the default settings, we score around 67% with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cv_mnnb = Pipeline(steps=[('cv', CountVectorizer()),\n",
    "                               ('mnnb', MultinomialNB())],\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cv_mnnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Pipeline 1 is 0.6709166428204129\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(pipe_cv_mnnb, 'Pipeline 1', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning\n",
    "\n",
    "In order to tune this model, we ran a GridSearch through a number of parameters, namely, whether or not to use stopwords in the text, and the range of n-grams we wanted to look at. \n",
    "\n",
    "Because grid searches take time to iterate through options, we have commented out the cells relating to the grid search and it's feedback, and have preserved the best scoring models. If you'd like to confirm, feel free to uncomment the grid search cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'cv__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3),(1,4),(2,4),(3,4),(4,4)],\n",
    "              'cv__stop_words': [None, nltk_sw]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid1 = GridSearchCV(pipe_cv_mnnb, param_grid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid1.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_from_grid1 = Pipeline([('cv', CountVectorizer(ngram_range= (1, 2))),\\\n",
    "                        ('mnnb', MultinomialNB())],\n",
    "                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.3s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_from_grid1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.3s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Best From CV/MNNB, Grid 1 is 0.6860228321603845\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(best_from_grid1, 'Best From CV/MNNB, Grid 1', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a promising model! It ran quickly and came out around 8% more accurate than our baseline. It's relatively simple, keeping in stopwords and adopting a simple monogram and bigram approach to n-grams.\n",
    "\n",
    "(During early modeling stages, we ran a grid checking a few other parameters, including the min/max df parameters. In this case, tweaking these parameters invariably dropped our accuracy to well below the default CountVectorizer and MNNB model. We've left those out of the main notebook for simplicity's sake.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Classifiers: the Random Forest\n",
    "\n",
    "We tried a few other models for classification. One of these was a Random Forest model. A Random Forest model is a little more complicated than a Naive Bayes model, potencially offering clearer results, at the cost of computaional demands.\n",
    "\n",
    "Here's an attempt with a Random Forest classifier, using default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_count_vec_rf =Pipeline([('countvec', CountVectorizer()), \n",
    "                              ('rf', RandomForestClassifier(random_state = 42))] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "The cv mean of Default CV and RF is 0.6761965261207918\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(pipeline_count_vec_rf, 'Default CV and RF', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad results, although considerably slower than the Naive Bayes.  We next tried a grid search through a number of parameters. \n",
    "\n",
    "We tried the same basic parameters tweaking the CountVectorizer as before. Additionally, we're testing parameters for the RandomForest model like n_estimators (how many trees in the forest) and max features (how many features the descion tree looks at when making a split). \n",
    "\n",
    "As before, we've commented out the actual GridSeach because it takes a long time to run. If you'd like to check our results, feel free to uncomment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid4dict = {\n",
    "   'countvec__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)],\n",
    "   'countvec__stop_words': [None, 'english', nltk_sw],\n",
    "   'rf__n_estimators': [50, 100, 150],\n",
    "   'rf__max_features': ['auto', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid4 = GridSearchCV(pipeline_count_vec_rf, grid4dict, random_state=42, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid4.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grid4.best_estimator_)\n",
    "#grid4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite a wider range of options, the best model here is not far off from  default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfromgrid4 = Pipeline(steps=[\n",
    "                ('countvec', CountVectorizer(stop_words = nltk_sw)),\n",
    "                ('rf', RandomForestClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec',\n",
       "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier(random_state=42))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestfromgrid4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cv mean of Best From RandomForest, Grid 4 is 0.6838230959393954\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestfromgrid4, 'Best From RandomForest, Grid 4', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty good result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf\n",
    "\n",
    "Tf-idf is an alternate way to vectorize text data. Rather than simply counting the number of times a word appears in a document, it compares how often a word appears in a document in relation to how often that word appears in the whole dataset. That is, a word that appears many times in a document but also many times in the corpus as a whole will be given less importance than a word that appears many times in that document but rarely appears in the overall corpus. Below, we paired this vectorizer with the MNNB model we've used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tfidf_mnnb = Pipeline([('Tfidf', TfidfVectorizer()), \n",
    "                              ('mnnb', MultinomialNB())] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Default Tfidf/Naive Bayes is 0.6495082651473914\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(pipeline_tfidf_mnnb, 'Default Tfidf/Naive Bayes', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooo, much worse than the basic CountVectorizer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2dict = {\n",
    "   'Tfidf__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3),(1,4),(2,4),(3,4),(4,4)],\n",
    "   'Tfidf__stop_words': [None, 'english', nltk_sw]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2 = GridSearchCV(pipeline_tfidf_mnnb, grid2dict, verbose=1)\n",
    "#grid2.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfromgrid2 = Pipeline(steps=[('Tfidf',\n",
    "                 TfidfVectorizer(ngram_range=(2, 2),\n",
    "                                 stop_words= nltk_sw)),\n",
    "                ('mnnb', MultinomialNB())],\n",
    "         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing Tfidf, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Best from TF-IDF, Grid 2 is 0.6556678062665803\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestfromgrid2, 'Best from TF-IDF, Grid 2', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not great. TF-IDF might require more tweaking if it is to compete with our other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dummy': 0.6137264056559728,\n",
       " 'Pipeline 1': 0.6709166428204129,\n",
       " 'Best From CV/MNNB, Grid 1': 0.6860228321603845,\n",
       " 'Default CV and RF': 0.6761965261207918,\n",
       " 'Best From RandomForest, Grid 4': 0.6838230959393954,\n",
       " 'Default Tfidf/Naive Bayes': 0.6495082651473914,\n",
       " 'Best from TF-IDF, Grid 2': 0.6556678062665803}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although very close in accuracy to our Naive Bayes Model, the Random Forest model took a fairly long time to run.  In order to increase usability for our client, we opted to use the Best from Grid 1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 1 of 2) Processing cv, total=   0.3s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = Pipeline([('cv', CountVectorizer(ngram_range= (1, 2))),\\\n",
    "                        ('mnnb', MultinomialNB())],\n",
    "                       verbose = True)\n",
    "\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6704795424549054"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76      1359\n",
      "           1       0.60      0.48      0.53       763\n",
      "           2       0.78      0.14      0.24       151\n",
      "\n",
      "    accuracy                           0.67      2273\n",
      "   macro avg       0.69      0.48      0.51      2273\n",
      "weighted avg       0.67      0.67      0.65      2273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "report = classification_report(y_true = y_test, y_pred = y_pred )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1d0c5c276d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAEGCAYAAAC95YRPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAApdUlEQVR4nO3deZxVdf3H8dd7ZoABkWXYYhVUTJGUjFSsXBLXX7n8zKWsrCxzL8tfafZLsyx+mZW5ZGSWVu5WWikoKm6liIiKmIqCgCDIKssAs3x+f5wzeplmhjsb98zl/Xw8zmPO/Z7l+z2Xy+d+7/d8v9+jiMDMzLKhpNAFMDOz9zgom5lliIOymVmGOCibmWWIg7KZWYaUFboAHUHfitIYPrRToYuRWa+83qfQRci+tZWFLkHmrWHlsojo19LjDztou1i+oiavfZ95fuPkiDi8pXm1JwflPAwf2olpk4cWuhiZdcinv1joImReySPPFroImTcl7nyjNccvX1HDtMnD8tq3dOCrfVuTV3tyUDazohBALbWFLkarOSibWVEIgqrIr/kiyxyUzaxouKZsZpYRQVBTBNNGOCibWdGoxUHZzCwTAqhxUDYzyw7XlM3MMiKAKrcpm5llQxBuvjAzy4yAmo4fkx2Uzaw4JCP6Oj4HZTMrEqIGFboQreagbGZFIbnR56BsZpYJST9lB2Uzs8yodU3ZzCwbXFM2M8uQQNQUwRPuHJTNrGi4+cLMLCMCsSlKC12MVnNQNrOikAwecfOFmVlm+EafmVlGRIia6Pg15Y5/BWZmqVqU17Ilkm6QtFTSrJy0CkkPSHo1/ds7Z9uFkuZIelnSYTnpH5L0Qrrtl5K2mLmDspkVheRGX1leSx5+DxxeL+0C4MGIGAk8mL5G0ijgJGD39JhrJdXdcfwVcBowMl3qn/M/OCibWVGou9GXz7LFc0U8Cqyol3w0cGO6fiNwTE76rRGxMSLmAnOAvSUNBHpExL8iIoCbco5plNuUzaxo1OTfT7mvpOk5rydGxMQtHDMgIhYDRMRiSf3T9MHAkzn7LUzTqtL1+ulNclA2s6LQzBF9yyJibBtl3dA3QTSR3iQHZTMrGrXt2/tiiaSBaS15ILA0TV8IDM3ZbwiwKE0f0kB6k9ymbGZFIZmQqCSvpYXuAU5J108B7s5JP0lSF0kjSG7oTUubOtZI2jftdfH5nGMa5ZqymRWFQFS10TBrSbcAB5K0PS8ELgYmALdLOhWYDxwPEBEvSrodmA1UA2dFRE16qjNIenJ0Be5LlyY5KGfEFecN5akpPejVt5qJD78MwKN/68kfrngfC14t55f3vsIue1YC8O9nu3Hl/yS/lgL43Dff4iNHrAbg4b/04tarBiBBxYAqvn3VG/TsU9Ngnh1Vv4p1fOvMx6joVUltiHsf3IW/TBrF/vvM43OfmsmwQas4538/wSuv9333mBHDVvD1U/9Ft25VRC2c9d1PUFW1bX78b3xqNpVrS6mthZpqcc4RuxS6SG0igjYbPBIRn25k08GN7H8ZcFkD6dOB0c3Ju2CfSkkB/Cwivpm+Ph/oHhGXtOBcvYDPRMS1LTh2HjA2IpY199i2dOiJKzjqi8u4/GvD3k0bvusGvnf9PH757aGb7Tv8/ZVcPellSstg+ZIyzhj/fvY9JAnKv/reYH4z9d/07FPD9T8YyD2/68fnzn9rq15Le6upFb/+44eZM68PXcuruPZHf+OZFwYxb0Evvv+zg/j6l/+52f4lJbVccNZj/N81H+P1+RVs330DNdXbdsvdt47fiXdWFNuXUn4DQ7KukJ/MjcB/S+q7xT23rBdwZkMbcjpxZ9oH9l3H9r03r9EOG7mRoTtv/I99y7sFpen/p6qNJdSNEYoAQmyoLCEC1q0tpc/7qtq55FvfilXdmDOvDwCVGzox/82e9K1Yz/xFvVi4uOd/7D92j0W8Pr83r8+vAGDN2vL2viFkBRAkNeV8liwr5FdlNTAROA+4KHeDpH7AdUBdtfHrEfGEpEuAtRHx03S/WcAnSNp6dpI0E3gA+AdJG9BiYAwwStJfSe6QlgNX5tEnMdP+PaMbV3xjKEsXduZbV81/N0ifM2EBp398V8q71TJoxEbO/tHCpk/UwQ3ou4adh6/g33Ma/24fPHA1BPz4gvvp2WMDU/81gtv/9oGtWMqMCfGjW16HgH/8oQ/3/alPoUvUZjzJfetdAzwv6Sf10q8Efh4Rj0saBkwGdmviPBcAoyNiDICkA4G907S56T5fiogVkroCT0u6KyKWt92lbF277rWe30x9mfmvduHyrw3jwwe9Q0lp8Peb+nLN/S8zcIdNXHPRYG67agCf+fqSQhe3XZR3qeJ7503lVzftzfrKzo3uV1oS7P7+pZz93U+wcWMZP7loMq++3odnXxy0FUubHecdvTMrlnSiZ58qJtz6OgvmdGHWU90LXaxWC+RJ7lsrIt6RdBNwLlCZs2k8Se227nUPSds38/TTcgIywLmSjk3Xh5J0W2k0KEs6jWTMOsMGF/q7q3HDRm6kvFst814uT5ovgEHDNwFwwFGruO3qAQUsXfspLa3l4vMe5qEnduTxp3doct9lK7rxwksDeGdNOQDTZg5h5xErttmgvGJJJwBWL+/EE5N6susH1xdJUIaq/Oa1yLQs1PV/AZwKbJeTVgKMi4gx6TI4ItaQNHnklrm8ifOuq1tJa87j03PuCTy7hWOJiIkRMTYixvbrk61m6bfmd6amOllfsrATC18rZ8CQTfR9XxXzXyln1fKkvDMe3Z6hIzcUsKTtJfjmaU8wf1FP7rp39y3uPf35wYwYtpIunaspKallj93e4o03/7PteVvQpWsNXbereXf9QwesYd6/m/yv0IGImjyXLCv410rapHA7SWC+IU2+HzgbuBxA0piImAnMI2lDRtJewIh0/zVAUzXpnsDKiFgvaVdg3za+jFb78Rk78Py/urN6RRknf2gUn/vmW2zfu4ZrvzuY1cvL+N/P7chOu1fyo1teZ9a07bjt6hGUlUFJSXDOjxa+2+3t5G+8xfnHjqSsU9B/8CbO/8X8Al9Z29v9/Us5ZP/XeH1+b677cdIX/4bbPkSnshrO+sJT9OyxgR9+awqvzavgwgmHsnZdF+66d3euvuzvRCQ15WnPDt1CLsWpd79qLv7tPABKy4KH/9Kb6VN7FLZQbSRo9xF9W4UitjgUu30yltZGRPd0fQAwF/hJRFyS9si4hqQduQx4NCJOT9uD7wb6A08DHwWOiIh5km4G9iDpnP0P4PyIqAvgXYC/kkwG8jLQD7gkIqbm0yVu7J7lMW3ytvmfOB+HfPqLhS5C5pU88myhi5B5U+LOZ1ozH8WQ0T3jrNs/kte+39n9vlbl1Z4KVlOuC8jp+hKgW87rZcCJDRxTCRzayPk+Uy9pas62jcARjRw3vBnFNrOMilBR1JQL3nxhZtYWkht92br/0xIOymZWJIrjGX0OymZWFJIbfdnuWZEPB2UzKxoe0WdmlhEe0WdmljH5PBQ16xyUzawoREBVrYOymVkmJM0XDspmZpmR9Xkt8uGgbGZFwV3izMwyxc0XZmaZUgzP6HNQNrOikPS+8NwXZmaZ4MEjZmYZ4+YLM7OMcO8LM7OMce8LM7OMiBDVDspmZtnh5gszs4woljbljl/XNzNL1YbyWvIh6TxJL0qaJekWSeWSKiQ9IOnV9G/vnP0vlDRH0suSDmvpNTgom1lRqOun3BZBWdJg4FxgbESMBkqBk4ALgAcjYiTwYPoaSaPS7bsDhwPXSmrRSBYHZTMrGrUoryVPZUBXSWVAN2ARcDRwY7r9RuCYdP1o4NaI2BgRc4E5wN4tuQa3KZtZUYiA6vwnue8raXrO64kRMfG9c8Wbkn4KzAcqgfsj4n5JAyJicbrPYkn900MGA0/mnG9hmtZsDspmVjSacaNvWUSMbWxj2lZ8NDACWAXcIemzTZyvoYwj38LkclA2s6LQxnNfjAfmRsTbAJL+DOwHLJE0MK0lDwSWpvsvBIbmHD+EpLmj2dymbGZFI0J5LXmYD+wrqZskAQcDLwH3AKek+5wC3J2u3wOcJKmLpBHASGBaS67BNWUzKxptNSFRRDwl6U5gBlANPAtMBLoDt0s6lSRwH5/u/6Kk24HZ6f5nRURNS/J2UDazohDRtoNHIuJi4OJ6yRtJas0N7X8ZcFlr83VQNrMiIWry732RWQ7KZlY08mwvzjQH5TzMXtyPD/7wzEIXI7PKdmpRz59tSsUjhS5B8SuWuS8clM2sOETSrtzROSibWdHw46DMzDIifKPPzCxb3HxhZpYh7n1hZpYREQ7KZmaZ4i5xZmYZ4jZlM7OMCESte1+YmWVHEVSUHZTNrEj4Rp+ZWcYUQVXZQdnMikZR15QlXUUT3zsRcW67lMjMrAUCqK0t4qAMTG9im5lZtgRQzDXliLgx97Wk7SJiXfsXycysZYqhn/IWO/VJGidpNsmTXJG0p6Rr271kZmbNFXkuGZZPT+tfAIcBywEi4jlg/3Ysk5lZC4iI/JYsy6v3RUQskDa7kBY9OtvMrF1lvBacj3yC8gJJ+wEhqTNwLmlThplZZgREEfS+yKf54nTgLGAw8CYwJn1tZpYxynPJri3WlCNiGXDyViiLmVnrFEHzRT69L3aU9DdJb0taKuluSTtujcKZmTXLNtL74mbgdmAgMAi4A7ilPQtlZtZsdYNH8lkyLJ+grIj4Q0RUp8sfyfx3jZlti5JHQm15ybKm5r6oSFcflnQBcCtJMD4R+MdWKJuZWfMUQe+Lpm70PUMShOuu8qs52wL4QXsVysysJdSGtWBJvYDrgdEkMe9LwMvAbcBwYB5wQkSsTPe/EDiVZBzHuRExuSX5NjX3xYiWnNDMrCDa/ibelcCkiPhUOkajG/Ad4MGImJC2IFwAfFvSKOAkYHeSe29TJO0SEc0eaJfXiD5Jo4FRQHldWkTc1NzMzMzaT9vdxJPUg2Q6iS8ARMQmYJOko4ED091uBKYC3waOBm6NiI3AXElzgL2BfzU37y0GZUkXp4UYBdwLHAE8Djgom1m25F9T7ispd3riiRExMef1jsDbwO8k7UnSnPs1YEBELAaIiMWS+qf7DwaezDl+YZrWbPnUlD8F7Ak8GxFflDSApJ3FzCxbavPec1lEjG1iexmwF3BORDwl6UqSporGNFRFb1FjSj5BuTIiaiVVp1X6pSTfItYOOpdW89vP303nshpKS2qZ8tKOXPfo3uzSfxkXHfkoXTtXsWjV9lz01/Gs29SZI0a/win7znz3+JEDlvPp64/nlSV9C3cR7axzWTUTv3w3nUprKSup5cEXd2TiQx8G4IR9X+CEfWZRU1vC468M46rJ4xjY6x1u/9ptzF/WC4AXFgxgwj3b5kSH3/jZfPYZv4ZVy8r46sffX+jitK22neR+IbAwIp5KX99JEpSXSBqY1pIHksTDuv2H5hw/BFjUkozzCcrT07uQvyGpwq8FprUkszqSaoAX0vxfAk6JiPXNOH4Q8Mu0AX4MMCgi7k23HQWMiogJrSljoWyqKeW0Px5FZVUnykpquOGUv/LEa8P49mGP8/Mp+/HM/EEcvedLnDJuJtc+sjf3zdqF+2btAsDO/Zbz8xPuK+qADLCpupQzbjiKyk2dKC2p4fqv3M0/XxlGl07VHLDbPD599QlU1ZTSe7vKd495c0UPTr7m+AKWOhvuv62Ce37Xl/+5ckGhi9Iu2qr3RUS8JWmBpPdHxMvAwcDsdDkFmJD+vTs95B7gZkk/I7nRN5IWxsktDh6JiDMjYlVEXAccQhJAv9iSzHJURsSYiBgNbCKZ9ChvEbEoIj6VvhwDHJmz7Z6OGpATorKqEwBlJUlNMELs0GcVz8wfCMCTc4dy8K6v/8eRh49+lUkvjtyqpS0MUbkpfY9KaykrrSWA4/Z+kRsf/SBVNaUArFzXtYBlzKZZT3Vnzcoifl5y2w6zPgf4k6TnSeLMj0iC8SGSXiWJhxMAIuJFkpHPs4FJwFkt6XkBTQ8e2aupbRExoyUZNuAxYI90sMoNJE0j64HTIuJ5SQeQdE2B5O3cH+gD/J2kzedSoKukjwI/BroCY4GLgOeAHdPml24kfQx3BIYB1wD90ry+EhH/bqPrabUS1XLzqXcytGI1t00fzaxFA3htaQUH7jKPqa+M4JDdXmNAj7X/cdyho17jvNsPL0CJt74S1fKHM+9iSMVq7nhqNC8uHMAOfVczZofFnDF+GpuqS7ly0jhmv5nchxnUew1/PPMO1m3szK+m7M3MNwYW+Aos6yJiJkksqe/gRva/DListfk29ZV5RRPbAvh4azOXVEbSm2MS8H2Sm4nHSPo4Se+OMcD5JN86T0jqDmx4txARmyR9DxgbEWen5/xCum21pOeAA4CHgU8CkyOiStJE4PSIeFXSPsC19a9H0mnAaQCduvdu7aU2S22UcNL1J9C9y0Z+dvwkduq3nEv+fhDfOuxxvvKx6TzyynCqajb/kTN60BI2VJXx2tt9tmpZC6U2Sjj5muPpXr6Ryz8zmZ36r6C0pJbtu27ki78+llGDl/Kjkx7gmCs+w7I12/HJyz/L6spydh30Nj89eRIn/vJE1m3sXOjLsDbWloNHCqWpwSMHtWO+XSXNTNcfA34LPAUcl+b9kKQ+knoCTwA/k/Qn4M8RsbDeU1CachvJsPCHSTp2X5sG9v2AO3LO06X+gWn3mIkA3foPLcg/9dqNXZj+xiD222kBf3hyDGfe/EkAhlWs4mM7z99s38N2n8OkF3cuRDELau2GLjwzdxDjRs5n6eruPDx7BCBmvzmACNGr2wZWre/K6sqkSePfi/qxcEUPhvVZxUuL+jd9cutYgqIYZp3PhETtoa5NeUxEnJN2zG6wS0naPvxlkmaJJyXt2ox87gGOSJtGPgQ8RHLNq3LyHxMRu7XyetpM726VdO+yEYAuZdXsM2Ih85b1one35D6oCL7y0We4c8aod48RwSG7vcbkbaI9GXp1q6R7+Xvv0d47LWTest5MfWk4H94xueE9rM8qOpXWsGp9Ob26VVKipK/U4N7vMLTPat5c2aNg5bd2VARTd2apxf9Rksn0fyDpQJJ+hO9I2ikiXgBekDQO2BWYmXPcGmD7hk4YEWslTSNpk/572vD+jqS5ko6PiDuUVJf3SB8IW3B9u6/n0qMeokS1lCh44KWdeWzOcD794ec5cewsAB76947c/dx730177bCIJe9sx5urto1A03f79Vxy3EOUlAQlCqbM2onHX96BstIavnfsVG495zaqakq55K6PA+KDwxdz+sFPU11bQm2ICXfvzzuV5VvMpxhdcO0b7DFuLT0rqvnj9Nn84YoBTL6leJq8iqH5QlGAeewkrY2I7vXSKoDfASPY/EbfVcBBJJN8zCYZ9jiQJMiOTo+bDHQi50ZfThvzp0jmgD4wIh5J00YAv0rP04lkeOSljZW3W/+hMfKEb7TV5Redsg1F8D+hnVXc0OzRttucKXHnM1sY0NGkLkOHxpCvn5fXvq+f/81W5dWe8hlmLZIa7I4RcamkYcD7IqLFfZXrB+Q0bQXJ+PH66ec0cIp5JDM31R334Xrbf59z/J3UaxqJiLnAttFNwWxbUgT1g3zalK8FxgGfTl+vIelOZmaWGYr8lyzLp015n4jYS9KzABGxMp3GzswsW4qg90U+QblKUinpDwNJ/WjOtB9mZltJ1mvB+cin+eKXwF+A/pIuI5m280ftWiozs5bYFrrERcSfJD1DMrRQwDER8VK7l8zMrDk6QHtxPvLpfTGMpIva33LTImJ+40eZmRXAthCUSZ5cXfcA1XKSfsQvkzyLyswsM1QEd7vyab74QO7rdPa4rzayu5mZtUKzh1lHxAxJ9QdrmJkV3rbQfCEpd3xxCckcxm+3W4nMzFpiW7nRx+aT/VSTtDHf1T7FMTNrhWIPyumgke4R8T9bqTxmZi1XzEFZUllEVDf1WCgzs6wQxd/7YhpJ+/FMSfeQTH+5rm5jRPy5nctmZpa/bahNuQJYTvIMu7r+ygE4KJtZthR5UO6f9ryYxXvBuE4RXLqZFZ0iiExNBeVSoDuNPDuvfYpjZtZyxd58sbipRySZmWVOkQfljj9btJltO6L4e18cvNVKYWbWFoq5ppw+kNTMrMMo9jZlM7OOxUHZzCwjOsCjnvKRzzP6zMwyTyTNF/kseZ9TKpX0rKS/p68rJD0g6dX0b++cfS+UNEfSy5IOa+l1OCibWdFo66AMfA3IfSbpBcCDETESeDB9jaRRwEkkT2Q6HLg2ndCt2RyUzax4tOHTrCUNAf4LuD4n+WjgxnT9RuCYnPRbI2JjRMwF5gB7t+QSHJTNrHjkH5T7Spqes5zWwNl+AXwLyO39PCAiFgOkf/un6YOBBTn7LUzTms03+sysODSvaWJZRIxtbKOkTwBLI+IZSQfmcb42m47CQdnMikfb9b74CHCUpCOBcqCHpD8CSyQNjIjFkgYCS9P9FwJDc44fAixqScZuvjCzoqHa/JYtiYgLI2JIRAwnuYH3UER8FrgHOCXd7RTg7nT9HuAkSV0kjQBGksxJ32yuKeeh0+pNDPrHgi3vuI2qXeLn6G5JdOpc6CJk36bWn2IrjOibANwu6VRgPnA8QES8KOl2YDbJs0zPioialmTgoGxmxaGdBo9ExFRgarq+nEbmBYqIy4DLWpufg7KZFY8iGNHnoGxmRaFuRF9H56BsZkVDtR0/Kjsom1lxKJIJiRyUzaxouPnCzCxLHJTNzLLDNWUzsyxxUDYzy4ht4GnWZmYdhvspm5llTXT8qOygbGZFwzVlM7Os8OARM7Ns8Y0+M7MMcVA2M8uKwDf6zMyyxDf6zMyyxEHZzCwbPHjEzCxLIjzJvZlZpnT8mOygbGbFw80XZmZZEYCbL8zMMqTjx2QHZTMrHm6+MDPLEPe+MDPLCs8SZ2aWHcngkY4flR2Uzax4eJY4M7PscE3Z2sXXLnqOvfdbwqqVXTjrswcA0L3HJi74wQz6D1zP0sXdmPDdvVi7pjMAx39+Dod+cj61NeLXP9+dGU/1L2Txt6pOnWu5/LbZdOoclJYGj0+q4I+/GEL3ntVceNWrDBiykSULu/Djs0ey9p1t8+Ped+BG/ufnc+ndr4qohXtv7sfdv3sfHztyBZ89702G7ryBrx01ildf2K7QRW2dNmxTljQUuAl4H0n9e2JEXCmpArgNGA7MA06IiJXpMRcCpwI1wLkRMbkleZe0uvSNkBSSrsh5fb6kS9ohn+/Ue/3Pts5ja5vyjyF877x9Nks7/nNzeG56X0474eM8N70vx3/uNQCGDl/D/uPf5IzPHMD3ztuHM8+fRUlJx68t5Ktqk7jg5N04678+wFmfGM2H9l/FrmPWcMLpi5j5z558+eNjmPnPnpxwxqJCF7VgamvEb344lNMO/gBfP2YUn/z8UoaNrGTeK135wVd3ZtZT2xe6iG0kmfsinyUP1cA3I2I3YF/gLEmjgAuAByNiJPBg+pp020nA7sDhwLWSSltyFe0WlIGNwH9L6tuOeQBsFpQjYr92zq/dvTizD2ve6bRZ2r4fW8KUe4cAMOXeIey7/1tJ+v5LeHTKYKqrSlmyuBuLFm7HLqNWbe0iF5DYsD757JeVBWVlQYQYd8hKptyVfPSm3NWXcYesLGQhC2rF0s7MmZXUgivXlbJgTlf6DNjEgjldWfh61wKXro1F5Lds8TSxOCJmpOtrgJeAwcDRwI3pbjcCx6TrRwO3RsTGiJgLzAH2bskltGdQrgYmAufV3yCpn6S7JD2dLh/JSX9A0gxJv5b0Rl1Ql/RXSc9IelHSaWnaBKCrpJmS/pSmrU3/3ibpyJw8fy/pOEmlki5P831e0lfb8T1oM70qNrJyeTkAK5eX06v3JgD69Ktk2ZLyd/db/nY5ffpVFqSMhVJSElz99xe45ekZPPtET15+rju9+lax8u2keWfl253p2aeqwKXMhgFDNrLT7ut5eWb3Qhel7UXyOKh8FqCvpOk5y2mNnVbScOCDwFPAgIhYDEngBuraCgcDC3IOW5imNVt7N7JdAzwv6Sf10q8Efh4Rj0saBkwGdgMuBh6KiB9LOhzIfaO+FBErJHUFnpZ0V0RcIOnsiBjTQN63AicC90rqDBwMnEHS5rM6Ij4sqQvwhKT702+3Dkf6z7SIBhKLWG2tOPsTH2C77av53+teYYdd1he6SJlU3q2G7143h19fOpT1a1v0yzr78r/Rtywixm5pJ0ndgbuAr0fEO2roP1y6a0Olybcwudo1KKcXcRNwLpBbfRsPjMq5wB6Stgc+ChybHjtJUu5vznMlHZuuDwVGAsubyP4+4Jdp4D0ceDQiKiUdCuwh6VPpfj3Tc20WlNNvztMAyksL3+a2akUXevfZwMrl5fTus4FVK5Na4LKlXek7YMO7+/Xpt4EVy8obO01RW7emjOef6sHY/VezalknevfbxMq3O9O73yZWL++05RMUsdKyWv73ujk8/Nc+PDGpotDFaT9teDtFUieSgPyniPhzmrxE0sCIWCxpILA0TV9IEpfqDAFadCOjPZsv6vyCpHaae2u3BBgXEWPSZXDabtPg15CkA0kC+biI2BN4Fmgy8kTEBmAqcBhJjfnWutMB5+TkPSIi7m/g+IkRMTYixnYuLXy721OPD2D8kQsBGH/kQp58bECS/tgA9h//JmWdahgwcD2Dh67jldm9CljSratnRRXbbV8NQOcutXzwI++w4PVynpzSm/HHLQNg/HHL+NcDvQtZzAILzvvJPObP6cqfr39foQvTrlRbm9eyxfMkNcbfAi9FxM9yNt0DnJKunwLcnZN+kqQukkaQVPSmteQa2r2PUNrkcDtJYL4hTb4fOBu4HEDSmIiYCTwOnAD8X1qjrfuf1BNYGRHrJe1Kcje0TpWkThHRUKPhrcCXgbHAF9K0ycAZkh6KiCpJuwBvRsS6trni1vvW92fwgb2W06PXJm68ewp/un4X7rhpZy647BkO+eR83l7SlR9f9CEA5s/dnscfHMR1Nz9CTY249qejqa3ddpovevev4vzLX6OkNJDgsXsrmPZQb16a0Z3vXD2Hw05YytuLunDZWSMLXdSC2X3sWsYft5y5L3XlmntnAfD7y4fQqXNwxvffoGdFNZf+7hVen92Niz7//gKXthWCthw88hHgc8ALkmamad8BJgC3SzoVmA8cDxARL6ZxbjbJ/bSzIqKmJRkr2qmztaS1EdE9XR9A0jzwk4i4JL15dw1JO3IZSdPC6ZL6A7eQBONHSGq4I9JT/pWk4fxloB9wSURMlfR/wFHAjIg4uV6+nYC3gHsi4otpWgnwQ+CTJLXmt4FjImJ1Y9fSs8uA2G/QyW311hSd2iVvF7oImRc1RTDUrJ09sOnmZ/Jp521Mz+0Gxb6j8rtvf//0S1qVV3tqt5pyXWBM15cA3XJeLyMJuPWtBg6LiGpJ44CDImJjuu2IRvL5NvDtRvKtAvrU27+W5Btvs650ZlYEPKKvzQ0j+WlQAmwCvlLg8phZR+Kg3LYi4lWS/oBmZs3Ttm3KBZOpoGxm1hr59KzIOgdlMysS+Q2hzjoHZTMrDoGDsplZpnT81gsHZTMrHp7k3swsSxyUzcwyIgKKYOSkg7KZFQ/XlM3MMsRB2cwsIwLI7/l7meagbGZFIiDcpmxmlg2Bb/SZmWWK25TNzDLEQdnMLCs8IZGZWXYE4Kk7zcwyxDVlM7Os8DBrM7PsCAj3UzYzyxCP6DMzyxC3KZuZZUSEe1+YmWWKa8pmZlkRRE1NoQvRag7KZlYcPHWnmVnGuEucmVk2BBCuKZuZZUR4knszs0wphht9iiLoQtLeJL0NvFHocuToCywrdCEyzu9R07L4/uwQEf1aerCkSSTXlY9lEXF4S/NqTw7KHZCk6RExttDlyDK/R03z+5NdJYUugJmZvcdB2cwsQxyUO6aJhS5AB+D3qGl+fzLKbcpmZhnimrKZWYY4KJuZZYiD8lYmKSRdkfP6fEmXtPBcvSSd2cJj50nKt0/nViOpRtJMSbMk3SGpWzOPHyTpznR9jKQjc7YdJemCti5ze2vLz8wW8vlOvdf/bOs8bMsclLe+jcB/t1FA7AU0GJQllbbB+QuhMiLGRMRoYBNwenMOjohFEfGp9OUY4MicbfdExIQ2K+nW05afmaZsFpQjYr92zs8a4KC89VWT3Pk+r/4GSf0k3SXp6XT5SJp+iaTzc/abJWk4MAHYKa1ZXi7pQEkPS7oZeCHd96+SnpH0oqTTtsYFtqHHgJ0lVaTX8bykJyXtASDpgPTaZ0p6VtL2koan709n4FLgxHT7iZK+IOlqST3TXwol6Xm6SVogqZOknSRNSt+zxyTtWsDrr9OSz0w/SQ9ImiHp15LeqAvqDX0mJE0Auqbv1Z/StLXp39vq/eL4vaTjJJWmn7un03+br7b7O7EtiAgvW3EB1gI9gHlAT+B84JJ0283AR9P1YcBL6folwPk555gFDE+XWTnpBwLrgBE5aRXp367pcX3S1/OAvoV+Pxp6f9K/ZcDdwBnAVcDFafrHgZnp+t+Aj6Tr3dNj3n1PgC8AV+ec+93X6bkPStdPBK5P1x8ERqbr+wAPZeE9acFn5mrgwnT9cJJJ1Ppu4TOxtpF/i2OBG9P1zsCC9NjTgO+m6V2A6bmfPS8tWzwhUQFExDuSbgLOBSpzNo0HRkmqe91D0vbNPP20iJib8/pcScem60OBkcDyFhR7a+kqaWa6/hjwW+Ap4DiAiHhIUh9JPYEngJ+lNbs/R8TCnPduS24jCcYPAycB10rqDuwH3JFzni6tv6TWa8Fn5qMkwZSImCRpZc4xzf1M3Af8UlIXkgD/aERUSjoU2ENSXXNRz/Rccxs5j+XBQblwfgHMAH6Xk1YCjIuI3P90SKpm86am8ibOuy7nuANJ/tOOi4j1kqZu4dgsqIyIMbkJajjSRkRMkPQPknbjJyWNBzbkmc89wI8lVQAfAh4CtgNW1c8/Q35B/p+ZBr+dWvKZiIgN6X6HkXyR3VJ3OuCciJjczOuwJrhNuUAiYgVwO3BqTvL9wNl1LySNSVfnAXulaXsBI9L0NUBTNemewMr0P9+uwL5tUfYCeBQ4Gd4NKsvSmuNOEfFCRPwfyU/n+u2/jb4/EbEWmAZcCfw9Imoi4h1grqTj07wkac/2uKCWaOZn5nHghDTtUKB3mt7UZ6JKUqdGsr8V+CLwMaAuCE8Gzqg7RtIukrZr2dVZHQflwrqCzacaPBcYm940mc17PQ/uAirSn/VnAK8ARMRy4In0xtblDZx/ElAm6XngB8CT7XMZ7e4S0veF5ObmKWn619Nrf47kJ/199Y57mOSn/UxJJzZw3tuAz6Z/65wMnJqe80Xg6La7jDaR72fm+8ChkmYARwCLSb6kmvpMTASer7vRV8/9wP7AlIjYlKZdD8wGZkiaBfwa//puNQ+zNitCaftvTURUSxoH/CrDzTKWw99qZsVpGHB72u1vE/CVApfH8uSasplZhrhN2cwsQxyUzcwyxEHZzCxDHJStTaiVs7vVO9fv60aJSbpe0qgm9j1QUrMnzlEjs+Q1ll5vn7XNzGuzuUvMmuKgbG2lydnd1MJZ6yLiyxExu4ldDiQZGm1WFByUrT3Uze622ax1jc0qlo6cu1rS7HTYdP+6E0maKmlsun64klnPnpP0oJKZ8k4Hzktr6R9T47Om9ZF0v5LZ5H5NMkS4SWpihj1JV6RleVBSvzQtizPMWQfjfsrWpiSVkYwgm5Qm7Q2Mjoi5aWBbHREfTgc3PCHpfuCDwPuBDwADSEaJ3VDvvP2A3wD7p+eqiIgVkq4jmc3sp+l+NwM/j4jHJQ0jGQq8G3Ax8HhEXCrpv0hmONuSL6V5dAWelnRXOopyO2BGRHxT0vfSc59NMiLu9Ih4VdI+wLUks9qZ5c1B2dpKQ7O77cfms9Y1NqvY/sAtEVEDLJL0UAPn35dkdrK58O48EA1pbNa0/YH/To/9hzafNa0xjc2mVst7Q7P/CPxZGZ5hzjoWB2VrKw3N7gY5s9bRyKxiSiZQ39IoJuWxDzQ+axp5Hl+3/4HkP5tapPlmeYY56yDcpmxbU2Ozij0KnJS2OQ8EDmrg2H8BB0gakR5bkabXnwmusVnTcmeaO4L3Zk1rTFOzqZUAdbX9z5A0i2R6hjnrOByUbWtqbFaxvwCvkjzC6lfAI/UPjIi3SdqB/5zO4FbXfPA34Ni6G300PWva/umsaYcC87dQ1qZmU1sH7C7pGZI240vT9KzPMGcdgOe+MDPLENeUzcwyxEHZzCxDHJTNzDLEQdnMLEMclM3MMsRB2cwsQxyUzcwy5P8BxEQKROJeYLcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conmat = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "disp = ConfusionMatrixDisplay(conmat, display_labels = ['Neutral','Positive','Negative'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final accuracy score marked against the test data shows a slight dip in accuracy, indicating a possibilty of minor overfitting, but it's fairly minor.\n",
    "\n",
    "More troublesome is the confusion matrix, and our poor f1-score on negative tweets. The fact of the matter is that negative tweets make up a fairly small section of the dataset. While the model overall beats baseline performance, it doesn't do as well on this micro scale. \n",
    "\n",
    "We could tweak the thresholds here, and try to pull in more of the lower left corner (that is, tweets that were negative that our model labeled neutral), but the trade off there would be an increase in the top or middle right columns (things our model flags as negative but is in fact neutral or positive). Further refinement might help there.\n",
    "\n",
    "As is, the model does well enough to get the gist from a large dataset, and I would trust it to \n",
    "\n",
    "There are some ways to improve this model. It is possible that if we had more time to do hyperparameter tweaking or try different classifiers we might find a more suitable combination, but there is no guarentee. \n",
    "\n",
    "Additional preprocessing might help, although our gridsearches often found that the defaults worked very well. \n",
    "\n",
    "Additional human labeled data, especially of negative tweets, might help the model learn to classify that better.\n",
    "\n",
    "Finally, this is a general model based on Apple and Google products. Our clients, if they existed, might want a more specific model based on tweets that were specifically about their product, which might pick up on domain/product specific praise or condemnation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
