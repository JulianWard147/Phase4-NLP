{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  target  \n",
       "0                                      Negative emotion       2  \n",
       "1                                      Positive emotion       1  \n",
       "2                                      Positive emotion       1  \n",
       "3                                      Negative emotion       2  \n",
       "4                                      Positive emotion       1  \n",
       "...                                                 ...     ...  \n",
       "9088                                   Positive emotion       1  \n",
       "9089                 No emotion toward brand or product       0  \n",
       "9090                 No emotion toward brand or product       0  \n",
       "9091                 No emotion toward brand or product       0  \n",
       "9092                 No emotion toward brand or product       0  \n",
       "\n",
       "[9093 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding='unicode_escape')\n",
    "\n",
    "df['target'] = df.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "\n",
    "unique_emotions = df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts().index\n",
    "\n",
    "for emotion in unique_emotions:\n",
    "    if emotion == \"Positive emotion\":\n",
    "        df.target.replace(to_replace=emotion, value=1, inplace=True)\n",
    "    elif emotion == \"Negative emotion\":\n",
    "        df.target.replace(to_replace=emotion, value=2, inplace=True)\n",
    "    else:\n",
    "        df.target.replace(to_replace=emotion, value=0, inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early EDA revealed a single null row: nan in tweet text, nan in emotion, no emotion listed. Just dropping that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.tweet_text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet_text.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, these one tweet hiding in the data that isn't a str, which throws off the tokenizer. So..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet_text = df.tweet_text.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sw = stopwords.words('english')\n",
    "nltk_sw.extend(['sxsw', 'mention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.tweet_text\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_count_vec_mnnb =Pipeline([('countvec', CountVectorizer()), \n",
    "                              ('mnnb', MultinomialNB())] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec', CountVectorizer()), ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_count_vec_mnnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvdict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Pipeline 1 is 0.6709166428204129\n"
     ]
    }
   ],
   "source": [
    "def reportcrossval(model, modelname, xtrain, ytrain):\n",
    "    cvmean = cross_val_score(model, xtrain, ytrain).mean()\n",
    "    cvdict[modelname] = cvmean \n",
    "    print(f\"The cv mean of {modelname} is {cvmean}\") \n",
    "reportcrossval(pipeline_count_vec_mnnb, 'Pipeline 1', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1dict = {\n",
    "   'countvec__min_df': [.01,.05,.1,.15, None],\n",
    "   'countvec__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)],\n",
    "   'countvec__stop_words': [None, 'english', nltk_sw]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran grid1 below, it took a long time. I've commented it out so it won't try to run again, but the best score and params are preserved as bestmodelfromgrid1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid1 = GridSearchCV(pipeline_count_vec_mnnb, grid1dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid1.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid1.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfromgrid1 = Pipeline([('countvec', CountVectorizer(\n",
    "                                            min_df = 0.01,\n",
    "                                            stop_words = nltk_sw)), \n",
    "                              ('mnnb', MultinomialNB())] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec',\n",
       "                 CountVectorizer(min_df=0.01,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodelfromgrid1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Best from Grid 1 is 0.626043443929748\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestmodelfromgrid1, 'Best from Grid 1', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodelfromgrid = Pipeline([('countvec', CountVectorizer(\n",
    "                                            ngram_range= (1, 3),\n",
    "                                            stop_words = None)), \n",
    "                              ('mnnb', MultinomialNB())] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Test is 0.6858750212464741\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(testmodelfromgrid, 'Test', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like whatever was going on with min/max df really hurt the model. Rerunning a grid with defaults on those params. (Should be much faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2dict = {\n",
    "   'countvec__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3),(1,4),(2,4),(3,4),(4,4)],\n",
    "   'countvec__stop_words': [None, 'english', nltk_sw]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2 = GridSearchCV(pipeline_count_vec_mnnb, grid2dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfromgrid2 = Pipeline([('countvec', CountVectorizer(ngram_range=(1, 2))), \n",
    "                              ('mnnb', MultinomialNB())] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.3s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('mnnb', MultinomialNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodelfromgrid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.2s\n",
      "[Pipeline] .............. (step 2 of 2) Processing mnnb, total=   0.0s\n",
      "The cv mean of Best from Grid 2 is 0.6860228321603845\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestmodelfromgrid2, 'Best from Grid 2', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting out the grid search again, to prevent it from running every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar some more preprocessing, futzing with RegEx, stemming and lemming etc, this might be the best we can get with CountVectorizing. Now to try some different models, and tf dif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_count_vec_rf =Pipeline([('countvec', CountVectorizer()), \n",
    "                              ('rf', RandomForestClassifier(random_state = 42))] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   5.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec', CountVectorizer()),\n",
       "                ('rf', RandomForestClassifier(random_state=42))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_count_vec_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.7s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.7s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.7s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.7s\n",
      "The cv mean of Default CV and RF is 0.6761965261207918\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(pipeline_count_vec_rf, 'Default CV and RF', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a fair bit slower, but has decent results untuned. Going to run a modest gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3dict = {\n",
    "   'countvec__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)],\n",
    "   'countvec__stop_words': [None, 'english', nltk_sw]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3 = GridSearchCV(pipeline_count_vec_rf, grid3dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid3.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfromgrid3 = Pipeline([('countvec', CountVectorizer(stop_words= nltk_sw)), \n",
    "                              ('rf', RandomForestClassifier(random_state=42))] ,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   5.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec',\n",
       "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier(random_state=42))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodelfromgrid3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   4.0s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.9s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.9s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.9s\n",
      "[Pipeline] .......... (step 1 of 2) Processing countvec, total=   0.1s\n",
      "[Pipeline] ................ (step 2 of 2) Processing rf, total=   3.8s\n",
      "The cv mean of Best from Grid 3 is 0.6838230959393954\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestmodelfromgrid3, 'Best from Grid 3', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid4dict = {\n",
    "   'countvec__ngram_range': [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)],\n",
    "   'countvec__stop_words': [None, 'english', nltk_sw],\n",
    "   'rf__n_estimators': [50, 100, 150],\n",
    "   'rf__max_features': ['auto', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid4 = GridSearchCV(pipeline_count_vec_rf, grid4dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid4.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grid4.best_estimator_)\n",
    "#grid4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfromgrid4 = Pipeline(steps=[\n",
    "                ('countvec', CountVectorizer(stop_words = nltk_sw)),\n",
    "                ('rf', RandomForestClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec',\n",
       "                 CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('rf', RandomForestClassifier(random_state=42))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestfromgrid4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cv mean of Best From Grid 4 is 0.6838230959393954\n"
     ]
    }
   ],
   "source": [
    "reportcrossval(bestfromgrid4, 'Best From Grid 4', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pipeline 1': 0.6709166428204129,\n",
       " 'Best from Grid 1': 0.626043443929748,\n",
       " 'Test': 0.6858750212464741,\n",
       " 'Best from Grid 2': 0.6860228321603845,\n",
       " 'Default CV and RF': 0.6761965261207918,\n",
       " 'Best from Grid 3': 0.6838230959393954,\n",
       " 'Best From Grid 4': 0.6838230959393954}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
